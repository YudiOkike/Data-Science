xlab("Time (Weeks)") +
ylab("Fuel Price (Pence per Litre)") +
scale_colour_manual(name = "Series", values = c("Historical" = "grey70", "15-Week Forecast" = "darkgreen")) +
theme_minimal()
# Set 15-week forecast horizon
h_future <- 15
# Replicate last Brent price for next 15 weeks (assumption: constant exogenous input)
future_xreg <- rep(tail(xreg_external_full, 1), h_future)
# Generate forecast
forecast_best <- forecast(model_sarimax_best, xreg = future_xreg, h = h_future)
# Plot historical data with future forecast
autoplot(ts_full, series = "Historical") +
autolayer(forecast_best$mean, series = "15-Week Forecast", color = "darkgreen") +
ggtitle("15-Week Ahead Forecast – SARIMAX (Brent)") +
xlab("Time (Weeks)") +
ylab("Fuel Price (Pence per Litre)") +
scale_colour_manual(name = "Series", values = c("Historical" = "grey70", "15-Week Forecast" = "darkgreen")) +
theme_minimal()
# Plot historical data with future forecast
autoplot(ts_full, series = "Historical") +
autolayer(forecast_best$mean, series = "15-Week Forecast", color = "darkgreen") +
ggtitle("15-Week Ahead Forecast – SARIMAX (Brent)") +
xlab("Time (Weeks)") +
ylab("Fuel Price (Pence per Litre)") +
scale_colour_manual(name = "Series", values = c("Historical" = "grey70", "15-Week Forecast" = "darkgreen")) +
theme_minimal()
# Create table of future dates and predicted prices
future_forecast_df <- data.frame(
Week = seq(max(fuel_clean$Date) + 7, by = 7, length.out = h_future),
Forecast_Price = round(forecast_best$mean, 2)
)
# Display table
knitr::kable(future_forecast_df, caption = "10-Week Ahead Forecast – SARIMAX (Brent)")
# Create lagged features for Random Forest model
fuel_rf <- fuel_clean %>%
mutate(
lag_1 = lag(avg_price, 1),
lag_2 = lag(avg_price, 2),
lag_3 = lag(avg_price, 3)
) %>%
drop_na()
# Perform 80/20 train-test split
split_idx <- floor(0.8 * nrow(fuel_rf))
train_rf <- fuel_rf[1:split_idx, ]
test_rf  <- fuel_rf[(split_idx + 1):nrow(fuel_rf), ]
# Define predictors (lagged features) and target variable
x_train <- train_rf[, c("lag_1", "lag_2", "lag_3")]
y_train <- train_rf$avg_price
x_test <- test_rf[, c("lag_1", "lag_2", "lag_3")]
y_test <- test_rf$avg_price
# Initialize dataframe to store results
rf_results <- data.frame()
# Fit Random Forest models with varying number of trees
for (trees in c(100, 400, 700)) {
set.seed(42)  # Ensure reproducibility
rf_model <- randomForest(x = x_train, y = y_train, ntree = trees)
preds <- predict(rf_model, x_test)
# Calculate accuracy metrics
rmse_val <- sqrt(mean((preds - y_test)^2))
mae_val  <- mean(abs(preds - y_test))
mape_val <- mean(abs((preds - y_test) / y_test)) * 100
# Store results
rf_results <- rbind(rf_results, data.frame(
Model = paste("Random Forest (ntree =", trees, ")"),
RMSE = round(rmse_val, 4),
MAE  = round(mae_val, 4),
MAPE = round(mape_val, 4)
))
}
# Display formatted table of Random Forest accuracy metrics
knitr::kable(rf_results, caption = "Random Forest Forecast Accuracy for Varying ntree") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
# Fit Random Forest model with best ntree (700)
set.seed(42)
rf_best <- randomForest(x = x_train, y = y_train, ntree = 700)
rf_pred_best <- predict(rf_best, x_test)
# Prepare data for plotting
plot_df <- data.frame(
Week = 1:length(y_test),
Actual = y_test,
Predicted = rf_pred_best
)
# Plot actual vs predicted values for best Random Forest model
ggplot(plot_df, aes(x = Week)) +
geom_line(aes(y = Actual, colour = "Actual")) +
geom_line(aes(y = Predicted, colour = "Predicted"), linetype = "dashed") +
scale_colour_manual(values = c("Actual" = "red", "Predicted" = "blue")) +
labs(title = "Random Forest (ntree = 700): Actual vs Predicted",
y = "Fuel Price (Pence per Litre)", x = "Week") +
theme_minimal()
# Select relevant columns for LSTM model
fuel_lstm <- fuel_data %>% select(Date, avg_price)
# Normalise avg_price to [0,1] for better LSTM training
min_price <- min(fuel_lstm$avg_price)
max_price <- max(fuel_lstm$avg_price)
fuel_lstm$norm_price <- (fuel_lstm$avg_price - min_price) / (max_price - min_price)
# Function to create supervised learning dataset with lagged features
create_supervised <- function(series, lag = 3) {
x <- NULL; y <- NULL
for (i in 1:(length(series) - lag)) {
x <- rbind(x, series[i:(i + lag - 1)])
y <- c(y, series[i + lag])
}
list(x = x, y = y)
}
# Create supervised dataset with 3 lags
supervised <- create_supervised(fuel_lstm$norm_price)
# Perform 80/20 train-test split
split_index <- floor(0.8 * nrow(supervised$x))
train_x <- supervised$x[1:split_index, ]
train_y <- supervised$y[1:split_index]
test_x <- supervised$x[(split_index + 1):nrow(supervised$x), ]
test_y <- supervised$y[(split_index + 1):length(supervised$y)]
# Reshape data for LSTM input (samples, timesteps, features)
train_x <- array(train_x, dim = c(nrow(train_x), ncol(train_x), 1))
test_x <- array(test_x, dim = c(nrow(test_x), ncol(test_x), 1))
# Define sequential LSTM model with one LSTM layer and one dense layer
model_lstm <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(dim(train_x)[2], 1)) %>%
layer_dense(units = 1)
# Compile model with mean squared error loss and Adam optimizer
model_lstm %>% compile(loss = "mse", optimizer = "adam")
# Train model with validation split
history <- model_lstm %>% fit(
x = train_x, y = train_y, epochs = 50, batch_size = 8,
validation_split = 0.1, verbose = 0
)
# Calculate accuracy metrics for LSTM predictions
lstm_metrics <- data.frame(
RMSE = round(rmse(test_y_actual, predicted), 3),
MAE = round(mae(test_y_actual, predicted), 3),
MAPE = round(mape(test_y_actual, predicted) * 100, 2)
)
# Generate predictions on test set
predicted <- model_lstm %>% predict(test_x)
# Denormalise predictions to original scale
predicted <- predicted * (max_price - min_price) + min_price
# Denormalise actual test values
test_y_actual <- test_y * (max_price - min_price) + min_price
# Calculate accuracy metrics for LSTM predictions
lstm_metrics <- data.frame(
RMSE = round(rmse(test_y_actual, predicted), 3),
MAE = round(mae(test_y_actual, predicted), 3),
MAPE = round(mape(test_y_actual, predicted) * 100, 2)
)
# Display metrics
lstm_metrics
# Select relevant columns for LSTM model
fuel_lstm <- fuel_data %>% select(Date, avg_price)
# Normalise avg_price to [0,1] for better LSTM training
min_price <- min(fuel_lstm$avg_price)
max_price <- max(fuel_lstm$avg_price)
fuel_lstm$norm_price <- (fuel_lstm$avg_price - min_price) / (max_price - min_price)
# Function to create supervised learning dataset with lagged features
create_supervised <- function(series, lag = 3) {
x <- NULL; y <- NULL
for (i in 1:(length(series) - lag)) {
x <- rbind(x, series[i:(i + lag - 1)])
y <- c(y, series[i + lag])
}
list(x = x, y = y)
}
# Create supervised dataset with 3 lags
supervised <- create_supervised(fuel_lstm$norm_price)
# Perform 80/20 train-test split
split_index <- floor(0.8 * nrow(supervised$x))
train_x <- supervised$x[1:split_index, ]
train_y <- supervised$y[1:split_index]
test_x <- supervised$x[(split_index + 1):nrow(supervised$x), ]
test_y <- supervised$y[(split_index + 1):length(supervised$y)]
# Reshape data for LSTM input (samples, timesteps, features)
train_x <- array(train_x, dim = c(nrow(train_x), ncol(train_x), 1))
test_x <- array(test_x, dim = c(nrow(test_x), ncol(test_x), 1))
# Define sequential LSTM model with one LSTM layer and one dense layer
model_lstm <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(dim(train_x)[2], 1)) %>%
layer_dense(units = 1)
# Compile model with mean squared error loss and Adam optimizer
model_lstm %>% compile(loss = "mse", optimizer = "adam")
# Train model with validation split
history <- model_lstm %>% fit(
x = train_x, y = train_y, epochs = 50, batch_size = 8,
validation_split = 0.1, verbose = 0
)
# Generate predictions on test set
predicted <- model_lstm %>% predict(test_x)
# Denormalise predictions to original scale
predicted <- predicted * (max_price - min_price) + min_price
# Denormalise actual test values
test_y_actual <- test_y * (max_price - min_price) + min_price
# Calculate accuracy metrics for LSTM predictions
lstm_metrics <- data.frame(
RMSE = round(rmse(test_y_actual, predicted), 3),
MAE = round(mae(test_y_actual, predicted), 3),
MAPE = round(mape(test_y_actual, predicted) * 100, 2)
)
# Display metrics
lstm_metrics
# Select relevant columns for LSTM model
fuel_lstm <- fuel_data %>% select(Date, avg_price)
# Normalise avg_price to [0,1] for better LSTM training
min_price <- min(fuel_lstm$avg_price)
max_price <- max(fuel_lstm$avg_price)
fuel_lstm$norm_price <- (fuel_lstm$avg_price - min_price) / (max_price - min_price)
# Function to create supervised learning dataset with lagged features
create_supervised <- function(series, lag = 3) {
x <- NULL; y <- NULL
for (i in 1:(length(series) - lag)) {
x <- rbind(x, series[i:(i + lag - 1)])
y <- c(y, series[i + lag])
}
list(x = x, y = y)
}
# Create supervised dataset with 3 lags
supervised <- create_supervised(fuel_lstm$norm_price)
# Perform 80/20 train-test split
split_index <- floor(0.8 * nrow(supervised$x))
train_x <- supervised$x[1:split_index, ]
train_y <- supervised$y[1:split_index]
test_x <- supervised$x[(split_index + 1):nrow(supervised$x), ]
test_y <- supervised$y[(split_index + 1):length(supervised$y)]
# Reshape data for LSTM input (samples, timesteps, features)
train_x <- array(train_x, dim = c(nrow(train_x), ncol(train_x), 1))
test_x <- array(test_x, dim = c(nrow(test_x), ncol(test_x), 1))
# Define sequential LSTM model with one LSTM layer and one dense layer
model_lstm <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(dim(train_x)[2], 1)) %>%
layer_dense(units = 1)
# Compile model with mean squared error loss and Adam optimizer
model_lstm %>% compile(loss = "mse", optimizer = "adam")
# Train model with validation split
history <- model_lstm %>% fit(
x = train_x, y = train_y, epochs = 50, batch_size = 8,
validation_split = 0.1, verbose = 0
)
# Select relevant columns for LSTM model
fuel_lstm <- fuel_data %>% select(Date, avg_price)
# Normalise avg_price to [0,1] for better LSTM training
min_price <- min(fuel_lstm$avg_price)
max_price <- max(fuel_lstm$avg_price)
fuel_lstm$norm_price <- (fuel_lstm$avg_price - min_price) / (max_price - min_price)
# Function to create supervised learning dataset with lagged features
create_supervised <- function(series, lag = 3) {
x <- NULL; y <- NULL
for (i in 1:(length(series) - lag)) {
x <- rbind(x, series[i:(i + lag - 1)])
y <- c(y, series[i + lag])
}
list(x = x, y = y)
}
# Create supervised dataset with 3 lags
supervised <- create_supervised(fuel_lstm$norm_price)
# Perform 80/20 train-test split
split_index <- floor(0.8 * nrow(supervised$x))
train_x <- supervised$x[1:split_index, ]
train_y <- supervised$y[1:split_index]
test_x <- supervised$x[(split_index + 1):nrow(supervised$x), ]
test_y <- supervised$y[(split_index + 1):length(supervised$y)]
# Reshape data for LSTM input (samples, timesteps, features)
train_x <- array(train_x, dim = c(nrow(train_x), ncol(train_x), 1))
test_x <- array(test_x, dim = c(nrow(test_x), ncol(test_x), 1))
# Define sequential LSTM model with one LSTM layer and one dense layer
model_lstm <- keras_model_sequential() %>%
layer_lstm(units = 50, input_shape = c(dim(train_x)[2], 1)) %>%
layer_dense(units = 1)
# Compile model with mean squared error loss and Adam optimizer
model_lstm %>% compile(loss = "mse", optimizer = "adam")
# Train model with validation split
history <- model_lstm %>% fit(
x = train_x, y = train_y, epochs = 50, batch_size = 8,
validation_split = 0.1, verbose = 0
)
# Generate predictions on test set
predicted <- model_lstm %>% predict(test_x)
# Denormalise predictions to original scale
predicted <- predicted * (max_price - min_price) + min_price
# Denormalise actual test values
test_y_actual <- test_y * (max_price - min_price) + min_price
# Calculate accuracy metrics for LSTM predictions
lstm_metrics <- data.frame(
RMSE = round(rmse(test_y_actual, predicted), 3),
MAE = round(mae(test_y_actual, predicted), 3),
MAPE = round(mape(test_y_actual, predicted) * 100, 2)
)
# Display metrics
lstm_metrics
# Prepare data for plotting
plot_df <- data.frame(Week = 1:length(test_y_actual), Actual = test_y_actual, Predicted = predicted)
# Plot actual vs predicted values for LSTM model
ggplot(plot_df, aes(x = Week)) +
geom_line(aes(y = Actual, colour = "Actual")) +
geom_line(aes(y = Predicted, colour = "Predicted")) +
scale_colour_manual(values = c("Actual" = "red", "Predicted" = "blue")) +
labs(title = "LSTM Forecast vs Actual", y = "Fuel Price", x = "Week") +
theme_minimal()
# Create lagged features for SVR model
fuel_data <- fuel_data %>%
mutate(
lag_1 = lag(avg_price, 1),
lag_2 = lag(avg_price, 2),
lag_3 = lag(avg_price, 3)
) %>% drop_na()
# Select relevant columns
data_svr <- fuel_data %>% drop_na() %>% select(avg_price, lag_1, lag_2, lag_3)
# Perform 80/20 train-test split
split_index <- floor(0.8 * nrow(data_svr))
train_data <- data_svr[1:split_index, ]
test_data <- data_svr[(split_index + 1):nrow(data_svr), ]
# Center and scale training and test data
preproc <- preProcess(train_data, method = c("center", "scale"))
train_scaled <- predict(preproc, train_data)
test_scaled <- predict(preproc, test_data)
# Define predictors and target
X_train <- train_scaled %>% select(-avg_price)
y_train <- train_scaled$avg_price
X_test <- test_scaled %>% select(-avg_price)
y_test <- test_scaled$avg_price
# Define kernel types to test
kernels <- c("linear", "polynomial", "radial")
svr_results <- data.frame()
# Train and evaluate SVR models for each kernel
for (k in kernels) {
# Train SVR model
svr_model <- svm(x = X_train, y = y_train, kernel = k)
svr_preds_scaled <- predict(svr_model, X_test)
# Inverse scale predictions and actual values
avg_mean <- preproc$mean["avg_price"]
avg_sd   <- preproc$std["avg_price"]
final_preds <- svr_preds_scaled * avg_sd + avg_mean
actual_vals <- y_test * avg_sd + avg_mean
# Calculate accuracy metrics
rmse_val <- RMSE(final_preds, actual_vals)
mae_val  <- MAE(final_preds, actual_vals)
mape_val <- mean(abs((final_preds - actual_vals) / actual_vals)) * 100
# Store results
svr_results <- rbind(svr_results, data.frame(
Kernel = k,
RMSE = round(rmse_val, 3),
MAE  = round(mae_val, 3),
MAPE = round(mape_val, 2)
))
}
# Display formatted table of SVR accuracy metrics
knitr::kable(svr_results, caption = "SVR Forecast Accuracy by Kernel Type") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
# Refit best model (linear kernel)
svr_model <- svm(x = X_train, y = y_train, kernel = "linear")
svr_preds_scaled <- predict(svr_model, X_test)
# Inverse scaling
avg_mean <- preproc$mean["avg_price"]
avg_sd   <- preproc$std["avg_price"]
final_preds <- svr_preds_scaled * avg_sd + avg_mean
actual_vals <- y_test * avg_sd + avg_mean
# Generate predictions for test set using SVR model
svr_preds_scaled <- predict(svr_model, X_test)
# Extract scaling parameters
avg_mean <- preproc$mean["avg_price"]
avg_sd   <- preproc$std["avg_price"]
# Inverse scale predictions and actual values
final_preds <- svr_preds_scaled * avg_sd + avg_mean
actual_vals <- y_test * avg_sd + avg_mean
# Calculate accuracy metrics for SVR predictions
svr_metrics <- data.frame(
RMSE = round(RMSE(final_preds, actual_vals), 3),
MAE = round(MAE(final_preds, actual_vals), 3),
MAPE = round(mean(abs((final_preds - actual_vals) / actual_vals)) * 100, 2)
)
# Display metrics
svr_metrics
# Prepare data for plotting
plot_df <- tibble(Week = 1:length(actual_vals), Actual = actual_vals, Predicted = final_preds)
# Plot actual vs predicted values for SVR model
ggplot(plot_df, aes(x = Week)) +
geom_line(aes(y = Actual, colour = "Actual")) +
geom_line(aes(y = Predicted, colour = "Predicted"), linetype = "dashed") +
labs(title = "SVR Forecast vs Actual Fuel Prices", x = "Week", y = "Fuel Price") +
scale_colour_manual(values = c("Actual" = "black", "Predicted" = "blue")) +
theme_minimal()
# Align SVR and LSTM predictions to the same length
n_forecast <- min(length(test_y_actual), length(final_preds))  # LSTM vs SVR (linear)
# Create dataframe with aligned predictions and actual values
hybrid_df <- data.frame(
SVR = as.numeric(final_preds[1:n_forecast]),      # SVR predictions (re-scaled)
LSTM = as.numeric(predicted[1:n_forecast]),       # LSTM predictions (re-scaled)
Actual = as.numeric(test_y_actual[1:n_forecast])  # True values
)
# Create hybrid forecast by averaging SVR and LSTM predictions
hybrid_df <- hybrid_df %>%
mutate(Hybrid = 0.8 * SVR + 0.2 * LSTM) # Weighted in Favour of SVR
# Calculate accuracy metrics for hybrid forecast
hybrid_metrics <- data.frame(
RMSE = round(RMSE(hybrid_df$Hybrid, hybrid_df$Actual), 3),
MAE = round(MAE(hybrid_df$Hybrid, hybrid_df$Actual), 3),
MAPE = round(mean(abs((hybrid_df$Hybrid - hybrid_df$Actual) / hybrid_df$Actual)) * 100, 2)
)
# Display metrics
hybrid_metrics
# Prepare data for plotting
plot_df <- tibble(
Week = 1:n_forecast,
Actual = hybrid_df$Actual,
Hybrid = hybrid_df$Hybrid
)
# Plot actual vs hybrid forecast
ggplot(plot_df, aes(x = Week)) +
geom_line(aes(y = Actual, colour = "Actual")) +
geom_line(aes(y = Hybrid, colour = "Hybrid"), linetype = "dashed") +
labs(title = "Hybrid Forecast vs Actual (SVR + LSTM)",
x = "Week", y = "Fuel Price") +
scale_colour_manual(values = c("Actual" = "black", "Hybrid" = "darkgreen")) +
theme_minimal()
# Construct MAPE comparison dataframe
mape_values <- data.frame(
Model = c(
"Random Forest (700)",
"LSTM",
"SVR (Linear)",
"Hybrid (SVR + LSTM)",
"Auto SARIMAX (Brent)"
),
MAPE = c(
tail(rf_results$MAPE, 1),      # Best Random Forest
lstm_metrics$MAPE,
svr_metrics$MAPE,
hybrid_metrics$MAPE,
brent_metrics$MAPE             # From SARIMAX external model
)
)
# Plot histogram
ggplot(mape_values, aes(x = reorder(Model, -MAPE), y = MAPE, fill = Model)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = MAPE), vjust = -0.3) +
labs(
title = "MAPE Comparison of ML, Hybrid, and SARIMAX (Brent) Models",
x = "Model",
y = "MAPE (%)"
) +
theme_minimal() +
theme(legend.position = "none")
# --- Setup ---
# Get last 3 actual values from the dataset to serve as lags
last_obs <- tail(fuel_data, 3)
# Create input frame for preprocessing with dummy avg_price column (required by preproc)
future_lags <- data.frame(
avg_price = 0,
lag_1 = last_obs$avg_price[3],
lag_2 = last_obs$avg_price[2],
lag_3 = last_obs$avg_price[1]
)
# Scale lagged inputs
future_scaled <- predict(preproc, future_lags)
current_input <- as.numeric(future_scaled[, c("lag_1", "lag_2", "lag_3")])
# Forecast Loop
future_preds <- c()
for (i in 1:15) {
# Predict next value using SVR
pred_scaled <- predict(svr_model, as.data.frame(t(current_input)))
# Inverse scaling
pred_rescaled <- pred_scaled * preproc$std["avg_price"] + preproc$mean["avg_price"]
future_preds <- c(future_preds, pred_rescaled)
# Update lags (rolling forecast)
current_input <- c(pred_scaled, current_input[1:2])
}
# Format Output
future_dates <- seq(max(fuel_data$Date) + 7, by = 7, length.out = 15)
svr_forecast_15 <- data.frame(Date = future_dates, SVR_Prediction = round(future_preds, 2))
# Display forecast table
knitr::kable(svr_forecast_15, caption = "15-Week Ahead Forecast – SVR (Linear)")
# Combine historical and forecast data
historical_plot_df <- fuel_data %>%
select(Date, avg_price) %>%
rename(Price = avg_price)
forecast_plot_df <- svr_forecast_15 %>%
rename(Price = SVR_Prediction)
# Add source labels
historical_plot_df$Type <- "Historical"
forecast_plot_df$Type <- "Forecast"
# Merge for plotting
combined_plot_df <- bind_rows(historical_plot_df, forecast_plot_df)
# Plot: historical + forecast as lines only
ggplot(combined_plot_df, aes(x = Date, y = Price, colour = Type)) +
geom_line(size = 0.8) +
scale_colour_manual(values = c("Historical" = "skyblue", "Forecast" = "red")) +
labs(
title = "Historical Prices + 15-Week Ahead Forecast (SVR Linear)",
x = "Date",
y = "Fuel Price (Pence per Litre)",
colour = "Legend"
) +
theme_minimal(base_size = 13)
# Combine historical and forecast data
historical_plot_df <- fuel_data %>%
select(Date, avg_price) %>%
rename(Price = avg_price)
forecast_plot_df <- svr_forecast_15 %>%
rename(Price = SVR_Prediction)
# Add source labels
historical_plot_df$Type <- "Historical"
forecast_plot_df$Type <- "Forecast"
# Merge for plotting
combined_plot_df <- bind_rows(historical_plot_df, forecast_plot_df)
# Plot: historical + forecast as lines only
ggplot(combined_plot_df, aes(x = Date, y = Price, colour = Type)) +
geom_line(size = 0.8) +
scale_colour_manual(values = c("Historical" = "skyblue", "Forecast" = "red")) +
labs(
title = "Historical Prices + 15-Week Ahead Forecast (SVR Linear)",
x = "Date",
y = "Fuel Price (Pence per Litre)",
colour = "Colour"
) +
theme_minimal(base_size = 13)
